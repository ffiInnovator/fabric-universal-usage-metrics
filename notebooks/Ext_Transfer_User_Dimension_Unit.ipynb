{"cells":[{"cell_type":"markdown","source":["#### User Dimension\n","\n","##### Data ingestion strategy:\n","<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n","\n","##### Related pipeline(s):\n"," \n","**Ext_Load_PBI_Report_Usage_E2E**\n","\n","##### Source:\n","\n","**Files** from FUAM_Ext_Lakehouse folder **bronze_file_location** variable\n","\n","##### Target:\n","\n","**1 Delta table** in FUAM_Ext_Lakehouse \n","- **gold_table_name** variable value\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ce9545d0-788d-48ef-8984-c5c86c78097d"},{"cell_type":"code","source":["## Parameters\n","display_data = True\n","lakehouse_name = \"FUAM_Ext_Lakehouse\"\n","gold_table_name = \"users\"\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"3f774388-fd1c-44db-bb72-35a2c3a5f9ee"},{"cell_type":"code","source":["import json\n","from notebookutils import mssparkutils # type: ignore\n","from pyspark.sql import DataFrame, SparkSession # type: ignore\n","from pyspark.sql.types import StructType, StructField, StringType # type: ignore\n","import requests\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc583f41-23a8-44ca-afcf-2b185b67b4c5"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"TransferUserDimension\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"304d4039-61e3-4b60-b1d3-a6796cdd446f"},{"cell_type":"code","source":["def upsert_table(df: DataFrame, table_name: str, primary_key: str, lakehouse_name: str = None) -> int:\n","    \"\"\"\n","    Performs an upsert (merge) of the input DataFrame into a Delta Lake table.\n","\n","    Args:\n","        df (DataFrame): The input PySpark DataFrame to be upserted.\n","        table_name (str): The target Delta table name.\n","        primary_key (str): Column used as the primary key for matching rows.\n","        lakehouse_name (str, optional): Name of the lakehouse database.\n","\n","    Returns:\n","        int: Number of rows processed (from the input DataFrame).\n","    \"\"\"\n","    temp_view_name = \"temp_upsert_view\"\n","    df.createOrReplaceTempView(temp_view_name)\n","\n","    # Fully qualified table name\n","    qualified_table_name = f\"{lakehouse_name}.{table_name}\" if lakehouse_name else table_name\n","\n","    # Count rows in source DataFrame\n","    row_count = df.count()\n","\n","    # Check if table exists\n","    if spark._jsparkSession.catalog().tableExists(qualified_table_name):\n","        merge_sql = f\"\"\"\n","        MERGE INTO {qualified_table_name} AS target\n","        USING {temp_view_name} AS source\n","        ON target.{primary_key} = source.{primary_key}\n","        WHEN MATCHED THEN UPDATE SET *\n","        WHEN NOT MATCHED THEN INSERT *\n","        \"\"\"\n","        spark.sql(merge_sql)\n","    else:\n","        df.write.format(\"delta\").saveAsTable(qualified_table_name)\n","\n","    return row_count\n","\n","print(\"The function upsert_table has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"03d7ecd8-30e3-4e72-9d90-a1c2e9bcd898"},{"cell_type":"code","source":["#\n","# Get required secrets from the key vault\n","#\n","vault_uri = \"https://kv-fabric-dev-eastus2.vault.azure.net/\"\n","\n","# Retrieve secret from Key Vault using mssparkutils\n","TENANT_ID = mssparkutils.credentials.getSecret(vault_uri, \"TENANT-ID\")\n","CLIENT_ID = mssparkutils.credentials.getSecret(vault_uri, \"CLIENT-ID\")\n","CLIENT_SECRET = mssparkutils.credentials.getSecret(vault_uri, \"CLIENT-SECRET-KEY\")\n","\n","# Use the secret securely without printing\n","print(\"Secrets retrieved successfully (not displayed for security reasons).\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cd92d801-0330-44f3-a3cd-d4206fa22856"},{"cell_type":"code","source":["#\n","# Verify that key vault items cannot be viewed in clear text\n","#\n","print(f\"The value of the tenant ID is {TENANT_ID}\")\n","print(f\"The value of the client ID is {CLIENT_ID}\")\n","print(f\"The value of the client secret is {CLIENT_SECRET}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19c46eb9-8aa1-44dc-9e4b-1975af915524"},{"cell_type":"code","source":["#\n","# Connect to Fabric and get the authorization token\n","#\n","\n","# Auth config\n","scope = \"https://graph.microsoft.com/.default\"\n","token_url = f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/v2.0/token\"\n","\n","# Get access token\n","token_data = {\n","    \"client_id\": CLIENT_ID,\n","    \"client_secret\": CLIENT_SECRET,\n","    \"grant_type\": \"client_credentials\",\n","    \"scope\": scope\n","}\n","token_response = requests.post(token_url, data=token_data)\n","access_token = token_response.json()[\"access_token\"]\n","\n","print(f\"Access token retrieved successfully!\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bab8a69f-dca7-4b18-822d-4cdacf3420a0"},{"cell_type":"code","source":["#\n","# Call Graph API to get users\n","# NOTE: DEVIATION FROM FUAM STANDARD ARCHITECTURE\n","# We are doing this in a Notebook because Data Factory in Microsoft Fabric doesn't currently support a Web API or Web page connector in data pipelines.\n","# REF: https://learn.microsoft.com/en-us/fabric/data-factory/connector-web-overview\n","#\n","\n","headers = {\"Authorization\": f\"Bearer {access_token}\"}\n","users_url = \"https://graph.microsoft.com/v1.0/users?$select=id,displayName,userPrincipalName,mail,givenName,surname,officeLocation\"\n","\n","users = []\n","next_link = users_url\n","\n","while next_link:\n","    response = requests.get(next_link, headers=headers)\n","    data = response.json()\n","    users.extend(data.get(\"value\", []))\n","    next_link = data.get(\"@odata.nextLink\")\n","\n","# Load into Spark DataFrame\n","spark = SparkSession.builder.getOrCreate()\n","schema = StructType([\n","    StructField(\"id\", StringType(), True),\n","    StructField(\"displayName\", StringType(), True),\n","    StructField(\"userPrincipalName\", StringType(), True),\n","    StructField(\"mail\", StringType(), True),\n","    StructField(\"givenName\", StringType(), True),\n","    StructField(\"surname\", StringType(), True),\n","    StructField(\"officeLocation\", StringType(), True)\n","])\n","users_df = spark.createDataFrame(users, schema=schema)\n","\n","# Standardize column names\n","renamed_cols = [\n","    \"UserId\" if col_name == \"id\" else col_name[:1].upper() + col_name[1:]\n","    for col_name in users_df.columns\n","]\n","\n","# Apply the renamed columns to the DataFrame\n","users_df = users_df.toDF(*renamed_cols)\n","\n","print(\"Successfully created spark dataframe for 'users' table sourced fro Graph API.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e63c2c92-93cd-4ff2-8a93-55a1b445435f"},{"cell_type":"code","source":["if display_data:\n","    display(users_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"93edc20f-6a5d-43d0-a85f-8c6294eee697"},{"cell_type":"code","source":["# Upsert the flattened DataFrame into the Microsoft Fabric Lakehouse\n","# \tðŸ”„ Update rows where WorkspaceId matches\n","# \tâž• Insert new rows not already present\n","# \tâœ… Leave unmatched rows untouched\n","rows_processed = upsert_table(users_df, table_name=gold_table_name, primary_key=\"UserId\", lakehouse_name=lakehouse_name)\n","\n","print(f\"Upsert process completed successfully into table {gold_table_name} w/ {rows_processed} rows processed.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17b949ae-4209-4884-a151-b4cc39187c27"},{"cell_type":"code","source":["#\n","# Stop the Spark session\n","# NOTE: frees up limited F2 SKU capacity resources\n","#\n","spark.stop()\n","\n","print(\"Spark session has been stopped successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"00e9cae6-6747-4810-a496-2cc57a0645d8"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7"}],"default_lakehouse":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7","default_lakehouse_name":"FUAM_Ext_Lakehouse","default_lakehouse_workspace_id":"572c83e2-ec60-4579-9648-10234b4a30d1"}}},"nbformat":4,"nbformat_minor":5}