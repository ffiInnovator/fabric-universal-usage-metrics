{"cells":[{"cell_type":"code","source":["## Parameters\n","display_data = True\n","table_name = 'report_views'\n","\n","print(f\" Table name for deduplification is set at {table_name}.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"acf3c3ee-04e3-45e2-a1aa-c72c9900ca59"},{"cell_type":"code","source":["## Import all packages used in this notebook\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number\n","from pyspark.sql import SparkSession # type: ignore"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9501696d-ec4f-45ff-beb0-530ba6b35555"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"DedupeFactTable\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c28b9bd5"},{"cell_type":"code","source":["#\n","# Remove duplicates from the fact table\n","#\n","\n","# Load the table\n","df = spark.read.table(table_name)\n","\n","# Define a window to identify duplicates\n","if table_name == 'report_views':\n","    window_spec = Window.partitionBy(\"ReportId\", \"CreationTime\", \"UserId\", \"OriginalConsumptionMethod\").orderBy(\"CreationTime\")\n","else:\n","    window_spec = Window.partitionBy(\"ReportId\", \"CreationTime\", \"UserId\", \"Client\").orderBy(\"CreationTime\")\n","\n","# Add row numbers and keep only the first occurrence of each duplicate\n","deduped_df = df.withColumn(\"rn\", row_number().over(window_spec)).filter(\"rn = 1\").drop(\"rn\")\n","\n","print(f\"{table_name} before deduping contains {df.count()} rows.\")\n","print(f\"{table_name} after deduping contains {deduped_df.count()} rows.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4c3ac615"},{"cell_type":"code","source":["if display_data:\n","    display(deduped_df)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e757b8de-ec11-495e-9b5f-fab5eafd86da"},{"cell_type":"code","source":["#\n","# Overwrite the original table with the deduplicated data\n","#\n","deduped_df \\\n","    .write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"mergeSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .partitionBy(\"CreationDate\", \"ReportId\") \\\n","    .saveAsTable(table_name)\n","\n","print(f\"{table_name} has been successfully rewritten without duplicates.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de385683-fb5d-44aa-b22d-2a494a02397c"},{"cell_type":"code","source":["#\n","# When Power BI connects to a Fabric Lakehouse in Import mode via the SQL Analytics endpoint, it may query a snapshot of the Delta table \n","# that hasn’t yet caught up with the latest physical data update. This is particularly true when:\n","# \t•\tYou’re writing to the Lakehouse using notebooks or pipelines.\n","# \t•\tThe updates are made via overwrite or non-transactional file-level operations.\n","# \t•\tPower BI’s import query pulls from a delta table snapshot, and the _delta_log has not fully committed or compacted.\n","#\n","# ✅ Recommendation\n","# \t1.\tForce a newer snapshot via the OPTIMIZE command after your Lakehouse update step to commit a clean version.\n","#\n","spark.sql(f\"OPTIMIZE {table_name}\")\n","\n","print(f\"Optimizing table {table_name} has been completed successfully\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"795fa494-46c0-4fe3-90a3-068cecd0fe66"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7","default_lakehouse_name":"FUAM_Ext_Lakehouse","default_lakehouse_workspace_id":"572c83e2-ec60-4579-9648-10234b4a30d1","known_lakehouses":[{"id":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7"}]}}},"nbformat":4,"nbformat_minor":5}