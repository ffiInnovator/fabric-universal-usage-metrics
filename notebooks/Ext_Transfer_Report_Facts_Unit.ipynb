{"cells":[{"cell_type":"markdown","source":["#### Report Usage Facts\n","\n","##### Data ingestion strategy:\n","<mark style=\"background: #88D5FF;\">**MERGE**</mark>\n","\n","##### Related pipeline:\n","\n","**Ext_Load_PBI_Report_Usage_E2E**\n","\n","##### Source:\n","\n","**Files** from FUAM_Ext_Lakehouse folder **bronze_file_location** variable\n","\n","##### Target:\n","\n","**1 Delta table** in FUAM_Ext_Lakehouse \n","- **gold_table_name** variable value\n"],"metadata":{},"id":"e6f3fbdd-e0f2-48ce-82f6-a393d512149e"},{"cell_type":"code","source":["## Parameters\n","display_data = True\n","\n","usage_table_name = \"Report views\"\n","bronze_file_location = \"Files/raw/report_usage/facts/report_views/\"\n","gold_table_name = \"report_views\"\n","usage_dataset_id = \"a4c91678-f53c-479b-b3d2-bf573ef36660\"\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"d9a33d27-0cb3-4fce-88ac-0c112ce659bd"},{"cell_type":"code","source":["## Import all packages used in this notebook\n","import datetime\n","from delta.tables import DeltaTable\n","from pyspark.sql.functions import col, explode, to_date, date_format, lit, upper, row_number # type: ignore\n","from pyspark.sql.window import Window # type ignore\n","from pyspark.sql import SparkSession # type: ignore\n","import random\n","import re\n","import time\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10a066cc-e3ea-4049-8c46-e35cc24596a0"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"TransferReportFacts\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2cc01fe-7549-473b-88c8-2f438291131c"},{"cell_type":"code","source":["#\n","# Function to get check if the gold layer table already exists\n","#\n","def gold_table_exists(gold_table_name: str, spark) -> bool:\n","    \"\"\"\n","    Checks if a table exists in the FUAM_Ext_Lakehouse catalog.\n","\n","    Args:\n","        gold_table_name (str): Name of the table to check.\n","        spark (SparkSession): The active Spark session.\n","\n","    Returns:\n","        bool: True if the table exists, False otherwise.\n","    \"\"\"\n","    table_exists = spark._jsparkSession.catalog().tableExists('FUAM_Ext_Lakehouse', gold_table_name)\n","    return table_exists\n","\n","print(\"The function 'gold_table_exists' has been created successfully.\") "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe71ddf4-7352-4354-9dec-5b8a9df536be"},{"cell_type":"code","source":["#\n","# Function to get the refresh date for UPSERT processing from the SILVER and GOLD layers\n","#\n","def get_refresh_date(gold_table_name: str, dataset_id: str, silver_df, spark) -> datetime:\n","    spark = silver_df.sparkSession  # Get SparkSession from the DataFrame\n","\n","    # Get earlist date from silver_df\n","    silver_min_df = silver_df.select(col('CreationDate')).orderBy(col('CreationDate'), ascending=True).first()\n","    silver_earlist_date = silver_min_df['CreationDate'] if silver_min_df else None\n","\n","    if gold_table_exists(gold_table_name, spark):\n","        # Get latest date from gold table\n","        get_latest_date_sql = f\"\"\"\n","            SELECT CreationDate \n","            FROM FUAM_Ext_Lakehouse.{gold_table_name}\n","            WHERE UsageDatasetId = '{dataset_id}'\n","            ORDER BY CreationDate DESC \n","            LIMIT 1\n","        \"\"\"\n","        gold_max_df = spark.sql(get_latest_date_sql)\n","\n","        if gold_max_df.count() == 0:\n","            print(\"No existing records in gold table. Using date from silver_df.\")\n","            refresh_date = silver_earlist_date\n","        else:\n","            print(\"Using date from gold.\")\n","            refresh_date = gold_max_df.first()['CreationDate']\n","    else:\n","        print(\"Using date from silver_df.\")\n","        refresh_date = silver_earlist_date\n","\n","    print(f\"Refresh start date: {refresh_date}\")\n","    return refresh_date\n","\n","print(\"The function 'get_refresh_date' has been created successfully.\") "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0077e886-ff77-40cd-b465-ee8e4d13658a"},{"cell_type":"code","source":["#\n","# Get the report usage data from the BRONZE layer\n","# Read the JSON files using 'multiline' since it's pretty-printed\n","#\n","raw_location = f\"{bronze_file_location}{usage_dataset_id.upper()}.json\"\n","bronze_df = spark.read.option(\"multiline\", \"true\").json(raw_location)\n","\n","print(f\"Bronze data from {raw_location} has been read successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e2d4ffc0-87b9-4f22-8ed0-4ca37e26e46f"},{"cell_type":"code","source":["if display_data:\n","    display(bronze_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"0aee684d-32a3-4072-b0f3-7787d62399ac"},{"cell_type":"code","source":["#\n","# Explode results -> tables -> rows\n","#\n","exploded_results = bronze_df.select(explode(\"results\").alias(\"result\"))\n","exploded_tables = exploded_results.select(explode(\"result.tables\").alias(\"table\"))\n","exploded_rows = exploded_tables.select(explode(\"table.rows\").alias(\"row\"))\n","\n","print(f\"Bronze data from {raw_location} has been extracted and transformed.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"416f1a4a-b9a6-491f-a553-2b8235f229d7"},{"cell_type":"code","source":["if display_data:\n","    display(exploded_rows)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"7cf4f0bd-d3d9-4d37-91f8-aec973979535"},{"cell_type":"code","source":["#\n","# Create the silver layer dataframe\n","# Dataset IDs in the Lakehouse are expected to be uppercase\n","#\n","usage_dataset_id = usage_dataset_id.upper()\n","\n","# Create the silver dataframe\n","silver_df = exploded_rows.select(\n","    lit(usage_dataset_id).alias(\"UsageDatasetId\"), # Add to enable the append processing logic\n","    col(\"row.*\")\n",")\n","\n","# Rename columns with brackets to just the inner name\n","for col_name in silver_df.columns:\n","    match = re.search(r\"\\[(.*?)\\]\", col_name)\n","    if match:\n","        new_col_name = match.group(1)\n","        silver_df = silver_df.withColumnRenamed(col_name, new_col_name)\n","\n","# Put selected ID columns to Upper Case\n","for co in silver_df.columns:\n","    if co in ['CapacityId','WorkspaceId', 'ReportId']:\n","        silver_df = silver_df.withColumn(co, upper(silver_df[co]))\n","\n","# Transformation to standardize the time column\n","if \"Timestamp\" in silver_df.columns and \"CreationTime\" not in silver_df.columns:\n","    silver_df = silver_df.withColumnRenamed(\"Timestamp\", \"CreationTime\")\n","\n","# Transform the time stamp string data field\n","silver_df = silver_df.withColumn(\"CreationDate\", to_date(col(\"CreationTime\").substr(1, 10), \"yyyy-MM-dd\")) \\\n","                    .withColumn(\"CreationDateKey\", date_format(col(\"CreationTime\"), \"yyyyMMdd\")) \\\n","                    .withColumn(\"CreationHour\", date_format(col(\"CreationTime\"), \"H\")) \\\n","                    .withColumn(\"CreationMinute\", date_format(col(\"CreationTime\"), \"mm\"))\n","\n","print(f\"Silver dataframe has been created successfully with {silver_df.count()} rows.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0b17bb5c-3388-4dfe-8285-85b0a49089db"},{"cell_type":"code","source":["#\n","# Transform the silver layer dataframe\n","# Get the date to start append processing\n","#\n","refresh_date = get_refresh_date(gold_table_name, usage_dataset_id, silver_df, spark) \n","    \n","# Filter silver_df data based on reresh date\n","silver_df = silver_df.filter(col(\"CreationDate\") >= lit(refresh_date))\n","\n","print(f\"Silver dataframe has been filtered successfully on/after {refresh_date} with {silver_df.count()} rows.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20b0f9f9-660c-4302-b38f-ce43d7770c11"},{"cell_type":"code","source":["if display_data:\n","    display(silver_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"6b963ac5-d071-4ab8-bb3d-34704ff45cb5"},{"cell_type":"code","source":["#\n","# Configure the MERGE process\n","#\n","max_retries = 5\n","retry_delay = 60  # max delay in seconds\n","success = False  # Flag to indicate success\n","\n","if gold_table_name == 'report_views':\n","    match_criteria = f\"\"\"\n","        target.ReportId = source.ReportId AND\n","        target.CreationTime = source.CreationTime AND\n","        target.UserId = source.UserId AND\n","        target.OriginalConsumptionMethod = source.OriginalConsumptionMethod\n","    \"\"\"\n","else:\n","    match_criteria = f\"\"\"\n","        target.ReportId = source.ReportId AND\n","        target.CreationTime = source.CreationTime AND\n","        target.UserId = source.UserId AND\n","        target.Client = source.Client\n","    \"\"\"\n","\n","# Replace all non-alphanumeric characters (except spaces) with nothing\n","match_criteria = match_criteria.replace('\\n', ' ').replace('\\r', ' ')\n","# Replace multiple spaces with a single space\n","match_criteria = re.sub(r'\\s+', ' ', match_criteria)\n","# Trim leading/trailing whitespace\n","match_criteria = match_criteria.strip()\n","\n","print(\"Successfully configured the merge process.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21b82d16-a8f6-49bb-9bdd-6e4b2ba93ee0"},{"cell_type":"code","source":["#\n","# De-dupe the the SILVER layer dataframe to avoid MERGE match issues\n","#\n","if gold_table_name == 'report_views':\n","    window_spec = Window.partitionBy(\"ReportId\", \"CreationTime\", \"UserId\", \"OriginalConsumptionMethod\").orderBy(\"CreationTime\")\n","else:\n","    window_spec = Window.partitionBy(\"ReportId\", \"CreationTime\", \"UserId\", \"Client\").orderBy(\"CreationTime\")\n","\n","\n","# Keep only the first row for each group of duplicates\n","deduped_df = silver_df.withColumn(\"row_num\", row_number().over(window_spec)).filter(\"row_num = 1\").drop(\"row_num\")\n","\n","print(f\"{gold_table_name} before deduping contains {silver_df.count()} rows.\")\n","print(f\"{gold_table_name} after deduping contains {deduped_df.count()} rows.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5830df05-22dd-4982-a927-1c5150819c75"},{"cell_type":"code","source":["#\n","# Perform the merge to insert new and update existing records (UPSERT approach)\n","#\n","\n","# Load the target Delta table\n","delta_table = DeltaTable.forName(spark, gold_table_name)\n","\n","# Perform the MERGE operation with auto-update and insert\n","for attempt in range(max_retries):\n","    try:\n","        delta_table.alias(\"target\") \\\n","            .merge(deduped_df.alias(\"source\"), match_criteria) \\\n","            .whenMatchedUpdateAll() \\\n","            .whenNotMatchedInsertAll() \\\n","            .execute()\n","        success = True\n","        break  # Exit loop if successful\n","    except Exception as e:\n","        if \"ConcurrentAppendException\" in str(e):\n","            wait_time = random.randint(1, retry_delay)\n","            print(f\"Retrying due to concurrent append conflict... Attempt {attempt + 1}, sleeping {wait_time} seconds\")\n","            time.sleep(wait_time)\n","        else:\n","            raise e  # Raise other errors\n","\n","if not success:\n","    print(f\"Merge operation for gold table {gold_table_name} failed after {max_retries} retries.\")\n","else:\n","    print(f\"Gold table {gold_table_name} has been merged successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78c430fc-6e88-4085-9297-b0988ffdb60e"},{"cell_type":"code","source":["#\n","# Write history of bronze files\n","#\n","raw_path = bronze_file_location.replace(\"*/\", '', )\n","history_path = raw_path.replace(\"Files/raw/\", \"Files/history/\")\n","mssparkutils.fs.cp(raw_path, history_path + datetime.datetime.now().strftime('%Y/%m/%d') + \"/\", True) # type: ignore\n","\n","print(f\"History data copied to {history_path} successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f7a1e9a8-7730-4d1f-b028-4d16df5b9117"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7"}],"default_lakehouse":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7","default_lakehouse_name":"FUAM_Ext_Lakehouse","default_lakehouse_workspace_id":"572c83e2-ec60-4579-9648-10234b4a30d1"},"environment":{}}},"nbformat":4,"nbformat_minor":5}