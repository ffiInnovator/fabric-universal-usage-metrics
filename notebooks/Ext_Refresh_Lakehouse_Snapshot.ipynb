{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e47ce9-7c61-4582-a932-9cc5ccd46d9a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Refresh Lakehouse Snapshot\n",
    "\n",
    "##### Data ingestion strategy:\n",
    "<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n",
    "\n",
    "##### Related pipeline:\n",
    "\n",
    "**Ext_Load_PBI_Report_Usage_E2E**\n",
    "\n",
    "##### Source:\n",
    "\n",
    "**Files** from FUAM_Ext_Lakehouse\n",
    "\n",
    "##### Target:\n",
    "\n",
    "**ALL Delta table** in FUAM_Ext_Lakehouse \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14b835-30e9-42ec-814b-5608501fc416",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "display_data = True\n",
    "lakehouse_name = \"FUAM_Ext_lakehouse\"\n",
    "\n",
    "print(\"Successfully configured all paramaters for this run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14af2cd-e5d5-4b1e-a23f-60e43eda5ad4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession # type: ignore\n",
    "\n",
    "print(\"Successfully imported all packages for this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292a05f7-e6e5-4433-81e3-0e2a051900ad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create the Spark session\n",
    "#\n",
    "app_name = \"RefreshLakehouseSnapshot\"\n",
    "\n",
    "# Get the current Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "print(f\"Spark session {app_name} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d22e8f-d5f7-471d-a167-f309a9ab4841",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# List tables in the specified lakehouse (assuming the lakehouse name is the database name)\n",
    "#\n",
    "spark.catalog.setCurrentDatabase(lakehouse_name)\n",
    "\n",
    "# Get the list of tables as a DataFrame\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "\n",
    "# Extract just the table names into a Python list\n",
    "table_names = [row.tableName for row in tables_df.collect()]\n",
    "\n",
    "print(f\"The list of tables fom lakehouse {lakehouse_name} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcb8aa-93e4-46b6-8de7-773980135d3f",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "if display_data:\n",
    "    display(tables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e1d90-0c7b-4bcd-ba84-56976925f52c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# When Power BI connects to a Fabric Lakehouse in Import mode via the SQL Analytics endpoint, it may query a snapshot of the Delta table \n",
    "# that hasn’t yet caught up with the latest physical data update. This is particularly true when:\n",
    "# \t•\tYou’re writing to the Lakehouse using notebooks or pipelines.\n",
    "# \t•\tThe updates are made via overwrite or non-transactional file-level operations.\n",
    "# \t•\tPower BI’s import query pulls from a delta table snapshot, and the _delta_log has not fully committed or compacted.\n",
    "#\n",
    "# ✅ Recommendation\n",
    "# \t1.\tForce a newer snapshot via the OPTIMIZE command after your Lakehouse update step to commit a clean version.\n",
    "#\n",
    "for table in table_names:\n",
    "    print(f\"Optimizing table {table} ...\")\n",
    "    spark.sql(f\"OPTIMIZE {table}\")\n",
    "\n",
    "print(f\"\\nAll {len(table_names)} tables in {lakehouse_name} have been committed to a clean version successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33dc2f0-2170-47da-969a-c6f55b3995bd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Stop the Spark session\n",
    "# NOTE: frees up limited F2 SKU capacity resources\n",
    "#\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session has been stopped successfully.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "a2655017-a58a-416b-b08c-c9fbbf6a8ac7",
    "default_lakehouse_name": "FUAM_Ext_Lakehouse",
    "default_lakehouse_workspace_id": "572c83e2-ec60-4579-9648-10234b4a30d1",
    "known_lakehouses": [
     {
      "id": "a2655017-a58a-416b-b08c-c9fbbf6a8ac7"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
