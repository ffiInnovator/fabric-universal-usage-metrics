{"cells":[{"cell_type":"markdown","source":["#### Report Usage Dimensions\n","\n","##### Data ingestion strategy:\n","<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n","\n","##### Related pipeline:\n","\n","**Ext_Load_PBI_Report_Usage_E2E**\n","\n","##### Source:\n","\n","**Files** from FUAM_Ext_Lakehouse folder **bronze_file_location** variable\n","\n","##### Target:\n","\n","**1 Delta table** in FUAM_Ext_Lakehouse \n","- **gold_table_name** variable value\n"],"metadata":{},"id":"e6f3fbdd-e0f2-48ce-82f6-a393d512149e"},{"cell_type":"code","source":["## Parameters\n","display_data = True\n","\n","usage_table_name = \"Reports\"\n","bronze_file_location = \"Files/raw/report_usage/dimensions/usage_reports/\"\n","gold_table_name = \"usage_reports\"\n","usage_dataset_id = \"28678a20-198b-4fa5-8cb2-d211f273af85\"\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"d9a33d27-0cb3-4fce-88ac-0c112ce659bd"},{"cell_type":"code","source":["import datetime\n","from delta.tables import * # type: ignore\n","from delta.exceptions import ConcurrentAppendException # type: ignore\n","from notebookutils import mssparkutils # type: ignore\n","from pyspark.sql.functions import col, explode, to_date, date_format, lit, upper # type: ignore\n","from pyspark.sql import SparkSession # type: ignore\n","import re\n","import time\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10a066cc-e3ea-4049-8c46-e35cc24596a0"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"TransferReportDimensions\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2cc01fe-7549-473b-88c8-2f438291131c"},{"cell_type":"code","source":["def gold_table_exists(gold_table_name: str, spark) -> bool:\n","    \"\"\"\n","    Checks if a table exists in the FUAM_Ext_Lakehouse catalog.\n","\n","    Args:\n","        gold_table_name (str): Name of the table to check.\n","        spark (SparkSession): The active Spark session.\n","\n","    Returns:\n","        bool: True if the table exists, False otherwise.\n","    \"\"\"\n","    table_exists = spark._jsparkSession.catalog().tableExists('FUAM_Ext_Lakehouse', gold_table_name)\n","    return table_exists\n","\n","print(\"The function 'gold_table_exists' has been created successfully.\") "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe71ddf4-7352-4354-9dec-5b8a9df536be"},{"cell_type":"code","source":["# Get the report usage data from the BRONZE layer\n","# Read the JSON files using 'multiline' since it's pretty-printed\n","raw_location = f\"{bronze_file_location}{usage_dataset_id.upper()}.json\"\n","bronze_df = spark.read.option(\"multiline\", \"true\").json(raw_location)\n","\n","print(f\"Bronze data from {raw_location} has been read successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e2d4ffc0-87b9-4f22-8ed0-4ca37e26e46f"},{"cell_type":"code","source":["if display_data:\n","    display(bronze_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"0aee684d-32a3-4072-b0f3-7787d62399ac"},{"cell_type":"code","source":["# Explode results -> tables -> rows\n","exploded_results = bronze_df.select(explode(\"results\").alias(\"result\"))\n","exploded_tables = exploded_results.select(explode(\"result.tables\").alias(\"table\"))\n","exploded_rows = exploded_tables.select(explode(\"table.rows\").alias(\"row\"))\n","\n","print(f\"Bronze data from {bronze_file_location} has been extracted and transformed.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"416f1a4a-b9a6-491f-a553-2b8235f229d7"},{"cell_type":"code","source":["# Check if exploded_rows is not empty before trying to expand row.* in subsequent notebook cells\n","# Exit early, if nescessay\n","num_exploded_rows = exploded_rows.count()\n","if num_exploded_rows == 0:\n","    mssparkutils.notebook.exit(f\"Nothing to do for bronze layer {usage_table_name}. Notebook completed early with success.\")\n","else:\n","    print(f\"The number of rows in the bronze layer {usage_table_name} is {num_exploded_rows}.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4812ee35-0577-4d82-beed-fc03adb224ab"},{"cell_type":"code","source":["if display_data:\n","    display(exploded_rows)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"7cf4f0bd-d3d9-4d37-91f8-aec973979535"},{"cell_type":"code","source":["# Create the silver dataframe\n","silver_df = exploded_rows.select(\n","    lit(usage_dataset_id).alias(\"UsageDatasetId\"), # Add to enable the append processing logic\n","    col(\"row.*\")\n",")\n","\n","# Rename columns with brackets to just the inner name\n","# Ensure the column name is compatible with Delta Lakeâ€™s restrictions in Microsoft Fabric\n","for col_name in silver_df.columns:\n","    match = re.search(r\"\\[(.*?)\\]\", col_name)\n","    if match:\n","        # Extract inner name\n","        inner_name = match.group(1)\n","        # Sanitize: remove invalid characters (could also replace spaces with underscores)\n","        sanitized_name = re.sub(r\"[ ,;{}()\\n\\t=]\", \"\", inner_name)\n","        # Rename column\n","        silver_df = silver_df.withColumnRenamed(col_name, sanitized_name)\n","\n","# Transformation to standardize the time column\n","if \"ReportGuid\" in silver_df.columns and \"ReportId\" not in silver_df.columns:\n","    silver_df = silver_df.withColumnRenamed(\"ReportGuid\", \"ReportId\")\n","\n","# Put selected ID columns to Upper Case\n","for co in silver_df.columns:\n","    if co in ['CapacityId','WorkspaceId', 'ReportId', 'UsageDatasetId']:\n","        silver_df = silver_df.withColumn(co, upper(silver_df[co]))\n","\n","print(f\"Silver dataframe has been created successfully with {silver_df.count()} rows.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0b17bb5c-3388-4dfe-8285-85b0a49089db"},{"cell_type":"code","source":["if display_data:\n","    display(silver_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"6b963ac5-d071-4ab8-bb3d-34704ff45cb5"},{"cell_type":"code","source":["# Write silver_df to gold delta table\n","#   \"selective\" REPLACE: only partitions for this dataset's dates being refreshed\n","#   ToDo: check on (\"overwriteSchema\", \"true\")\n","silver_df \\\n","    .write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"mergeSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .partitionBy(\"UsageDatasetId\") \\\n","    .saveAsTable(gold_table_name)\n","\n","print(f\"Gold table {gold_table_name} has been updated successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5a4c260-69dc-47af-bc2d-140fd54c5cbe"},{"cell_type":"code","source":["#\n","# Write history of bronze files\n","#\n","raw_path = bronze_file_location.replace(\"*/\", '', )\n","history_path = raw_path.replace(\"Files/raw/\", \"Files/history/\")\n","mssparkutils.fs.cp(raw_path, history_path + datetime.datetime.now().strftime('%Y/%m/%d') + \"/\", True) # type: ignore\n","\n","print(f\"History data copied to {history_path} successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f7a1e9a8-7730-4d1f-b028-4d16df5b9117"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7"}],"default_lakehouse":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7","default_lakehouse_name":"FUAM_Ext_Lakehouse","default_lakehouse_workspace_id":"572c83e2-ec60-4579-9648-10234b4a30d1"},"environment":{}}},"nbformat":4,"nbformat_minor":5}