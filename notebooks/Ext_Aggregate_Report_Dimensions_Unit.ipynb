{"cells":[{"cell_type":"markdown","source":["#### Aggregate Report Dimensions \n","\n","##### Data ingestion strategy:\n","<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n","\n","##### Related pipeline(s):\n","\n","**Ext_Load_PBI_Workspace_Datasets_E2E**\n","\n","##### Source:\n","\n","**Tables** from FUAM_Ext_Lakehouse table **gold_table_name** variable\n","\n","##### Target:\n","\n","**1 Delta table** in FUAM_Ext_Lakehouse \n","- **agg_gold_table_name** variable value"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"89c97590-d6d0-4022-ae6b-420284dfa356"},{"cell_type":"code","source":["## Parameters\n","display_data = True\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"6df42b6d-0867-4078-b962-9ed8402f311d"},{"cell_type":"code","source":["## Variables\n","lakehouse_name = \"FUAM_Ext_Lakehouse\"\n","gold_table_name = \"workspace_datasets\"\n","agg_gold_table_name = \"usage_workspaces\"\n","\n","print(\"Successfully configured all variables for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad0dce90-448d-44a5-8cef-a2bbb0d00d62"},{"cell_type":"code","source":["from delta.tables import DeltaTable # type: ignore\n","from pyspark.sql import functions as F # type: ignore\n","from pyspark.sql import DataFrame, SparkSession # type: ignore\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3f8d439a-ea10-463a-97d8-71e9a78d71c1"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"AggregateReportDimensions\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5655ea3-a690-4624-bcb2-ef7dea5d808c"},{"cell_type":"code","source":["def upsert_table(df: DataFrame, table_name: str, primary_key: str, lakehouse_name: str = None) -> int:\n","    \"\"\"\n","    Performs an upsert (merge) of the input DataFrame into a Delta Lake table.\n","\n","    Args:\n","        df (DataFrame): The input PySpark DataFrame to be upserted.\n","        table_name (str): The target Delta table name.\n","        primary_key (str): Column used as the primary key for matching rows.\n","        lakehouse_name (str, optional): Name of the lakehouse database.\n","\n","    Returns:\n","        int: Number of rows processed (from the input DataFrame).\n","    \"\"\"\n","    temp_view_name = \"temp_upsert_view\"\n","    df.createOrReplaceTempView(temp_view_name)\n","\n","    # Fully qualified table name\n","    qualified_table_name = f\"{lakehouse_name}.{table_name}\" if lakehouse_name else table_name\n","\n","    # Count rows in source DataFrame\n","    row_count = df.count()\n","\n","    # Check if table exists\n","    if spark._jsparkSession.catalog().tableExists(qualified_table_name): # type: ignore\n","        merge_sql = f\"\"\"\n","        MERGE INTO {qualified_table_name} AS target\n","        USING {temp_view_name} AS source\n","        ON target.{primary_key} = source.{primary_key}\n","        WHEN MATCHED THEN UPDATE SET *\n","        WHEN NOT MATCHED THEN INSERT *\n","        \"\"\"\n","        spark.sql(merge_sql) # type: ignor\n","    else:\n","        df.write.format(\"delta\").saveAsTable(qualified_table_name)\n","\n","    return row_count\n","\n","print(\"The function upsert_table has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ca75f35f-f634-408a-89c4-86dc1e6202e1"},{"cell_type":"code","source":["# Load the table\n","df = spark.read.table(gold_table_name)\n","\n","# Select only workspace-level columns\n","workspace_columns = [\n","    \"WorkspaceId\", \"WorkspaceName\", \"WorkspaceDescription\", \"HasWorkspaceLevelSettings\", \"State\", \"Type\",\n","    \"CapacityId\", \"IsOnDedicatedCapacity\", \"IsReadOnly\",\n","    \"fuam_modified_at\", \"fuam_deleted\"\n","]\n","\n","# Group by WorkspaceId and get the first (or max, etc.) of each other column\n","flattened_df = df.groupBy(\"WorkspaceId\").agg(\n","    *[F.first(col).alias(col) for col in workspace_columns if col != \"WorkspaceId\"]\n",")\n","\n","print(f\"The flatten data frame from the {gold_table_name} table has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4448f424-947a-43ab-93fe-2d340107415f"},{"cell_type":"code","source":["if display_data:\n","    display(flattened_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8e8efabe-93bb-4682-a517-416275f8d10a"},{"cell_type":"code","source":["#\n","# Upsert the flattened DataFrame into the Microsoft Fabric Lakehouse\n","# \tðŸ”„ Update rows where WorkspaceId matches\n","# \tâž• Insert new rows not already present\n","# \tâœ… Leave unmatched rows untouched\n","#\n","rows_processed = upsert_table(flattened_df, table_name=agg_gold_table_name, primary_key=\"WorkspaceId\", lakehouse_name=lakehouse_name)\n","\n","print(f\"Upsert process completed successfully into table {agg_gold_table_name} w/ {rows_processed} rows processed.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20a1c46e-efb7-4cc7-9829-b4a03ed65760"},{"cell_type":"code","source":["#\n","# Stop the Spark session\n","# NOTE: frees up limited F2 SKU capacity resources\n","#\n","spark.stop()\n","\n","print(\"Spark session has been stopped successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c7c7278-5eb0-41f6-aef7-39bc2b22b7db"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7"}],"default_lakehouse":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7","default_lakehouse_name":"FUAM_Ext_Lakehouse","default_lakehouse_workspace_id":"572c83e2-ec60-4579-9648-10234b4a30d1"}}},"nbformat":4,"nbformat_minor":5}