{"cells":[{"cell_type":"markdown","source":["#### Dataset Refresh Facts\n","\n","##### Data ingestion strategy:\n","<mark style=\"background: #88D5FF;\">**MERGE**</mark>\n","\n","##### Related pipeline:\n","\n","**Ext_Load_PBI_Dataset_Refresh_E2E**\n","\n","##### Source:\n","\n","**Files** from FUAM_Ext_Lakehouse folder **bronze_file_location** variable\n","\n","##### Target:\n","\n","**1 Delta table** in FUAM_Ext_Lakehouse \n","- **gold_table_name** variable value\n"],"metadata":{},"id":"e6f3fbdd-e0f2-48ce-82f6-a393d512149e"},{"cell_type":"code","source":["## Parameters\n","display_data = True\n","\n","bronze_file_location = \"Files/raw/dataset_refresh/\"\n","gold_table_name = \"dataset_refreshes\"\n","\n","# use case valid-data\n","# dataset_id = \"2cfa112f-405c-4367-876f-d5591a270bab\"\n","\n","# use case empty-array\n","# dataset_id = \"1ECDC217-48FF-4308-8320-CD30305E1F3C\"\n","\n","# use case service-exception\n","# dataset_id = \"0270E8BE-EA7A-47DC-B698-6FD6A8D1A372\"\n","\n","# use case expand-array-issue\n","dataset_id = \"5A03CB75-6CB9-410A-B5BC-13244F6C2D5E\"\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"d9a33d27-0cb3-4fce-88ac-0c112ce659bd"},{"cell_type":"code","source":["## Import all packages used in this notebook\n","import datetime\n","from delta.tables import DeltaTable\n","from notebookutils import mssparkutils # type: ignore\n","from pyspark.sql.functions import * # type ignore\n","from pyspark.sql.types import * # type ignore\n","from pyspark.sql.window import Window # type ignore\n","from pyspark.sql import SparkSession # type: ignore\n","import random\n","import re\n","import time\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10a066cc-e3ea-4049-8c46-e35cc24596a0"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"TransferDatasetRefreshes\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b2cc01fe-7549-473b-88c8-2f438291131c"},{"cell_type":"code","source":["#\n","# Function to get check if the gold layer table already exists\n","#\n","def gold_table_exists(gold_table_name: str, spark) -> bool:\n","    \"\"\"\n","    Checks if a table exists in the FUAM_Ext_Lakehouse catalog.\n","\n","    Args:\n","        gold_table_name (str): Name of the table to check.\n","        spark (SparkSession): The active Spark session.\n","\n","    Returns:\n","        bool: True if the table exists, False otherwise.\n","    \"\"\"\n","    table_exists = spark._jsparkSession.catalog().tableExists('FUAM_Ext_Lakehouse', gold_table_name)\n","    return table_exists\n","\n","print(\"The function 'gold_table_exists' has been created successfully.\") "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe71ddf4-7352-4354-9dec-5b8a9df536be"},{"cell_type":"code","source":["#\n","# Function to get the refresh date for UPSERT processing from the SILVER and GOLD layers\n","#\n","def get_refresh_date(gold_table_name: str, dataset_id: str, silver_df, spark) -> datetime:\n","    spark = silver_df.sparkSession  # Get SparkSession from the DataFrame\n","\n","    # Get earlist date from silver_df\n","    silver_min_df = silver_df.select(col('CreationDate')).orderBy(col('CreationDate'), ascending=True).first()\n","    silver_earlist_date = silver_min_df['CreationDate'] if silver_min_df else None\n","\n","    if gold_table_exists(gold_table_name, spark):\n","        # Get latest date from gold table\n","        get_latest_date_sql = f\"\"\"\n","            SELECT CreationDate \n","            FROM FUAM_Ext_Lakehouse.{gold_table_name}\n","            WHERE DatasetId = '{dataset_id}'\n","            ORDER BY CreationDate DESC \n","            LIMIT 1\n","        \"\"\"\n","        gold_max_df = spark.sql(get_latest_date_sql)\n","\n","        if gold_max_df.count() == 0:\n","            print(\"No existing records in gold table. Using date from silver_df.\")\n","            refresh_date = silver_earlist_date\n","        else:\n","            print(\"Using date from gold.\")\n","            refresh_date = gold_max_df.first()['CreationDate']\n","    else:\n","        print(\"Using date from silver_df.\")\n","        refresh_date = silver_earlist_date\n","\n","    print(f\"Refresh start date: {refresh_date}\")\n","    return refresh_date\n","\n","print(\"The function 'get_refresh_date' has been created successfully.\") "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0077e886-ff77-40cd-b465-ee8e4d13658a"},{"cell_type":"code","source":["#\n","# Explodes the 'refreshAttempts' array while preserving rows with empty arrays.\n","# Returns a flattened DataFrame with one row per attempt, or one row with nulls if none exist.\n","# \n","def flatten_refresh_attempts(input_df):\n","    \"\"\"\n","    Safely explodes the 'refreshAttempts' array of structs, preserving rows with empty arrays.\n","    Returns a DataFrame with attempt fields flattened and null-padded when missing.\n","    \"\"\"\n","    attempt_fields = [\"attemptId\", \"startTime\", \"endTime\", \"type\"]\n","    \n","    # Step 1: Separate rows with and without attempts\n","    df_with_attempts = input_df.filter(size(\"refreshAttempts\") > 0)\n","    df_without_attempts = input_df.filter(size(\"refreshAttempts\") == 0)\n","\n","    # Step 2: Explode only if refreshAttempts is an array of structs\n","    exploded_df = df_with_attempts.withColumn(\"attempt\", explode(\"refreshAttempts\"))\n","\n","    # Step 3: Check if 'attempt' is actually a struct and extract fields safely\n","    # Guard against attempt being STRING\n","    attempt_schema = exploded_df.select(\"attempt\").schema[0].dataType\n","    if isinstance(attempt_schema, StructType):\n","        for field in attempt_fields:\n","            exploded_df = exploded_df.withColumn(field, col(f\"attempt.{field}\"))\n","    else:\n","        # Fallback: skip extraction and pad fields with nulls\n","        for field in attempt_fields:\n","            exploded_df = exploded_df.withColumn(field, lit(None).cast(StringType()))\n","\n","    exploded_df = exploded_df.drop(\"attempt\", \"refreshAttempts\")\n","\n","    # Step 4: Pad missing fields for rows without attempts\n","    for field in attempt_fields:\n","        if field not in df_without_attempts.columns:\n","            df_without_attempts = df_without_attempts.withColumn(field, lit(None).cast(StringType()))\n","\n","    df_without_attempts = df_without_attempts.drop(\"refreshAttempts\")\n","\n","    # Step 5: Union both sides\n","    flat_df = exploded_df.unionByName(df_without_attempts)\n","\n","    return flat_df\n","\n","print(\"The function 'flatten_refresh_attempts' has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d634bfd1-35f1-4aca-a5b1-d6a4610c415b"},{"cell_type":"code","source":["#\n","# Parses 'serviceExceptionJson' column if present, extracting fields like 'errorCode'.\n","# Returns a DataFrame with 'errorCode' added and handles missing structures gracefully.\n","#\n","def extract_service_exception_fields(input_df):\n","    \"\"\"\n","    Parses 'serviceExceptionJson' column if present, extracting fields like 'errorCode'.\n","    Returns a DataFrame with 'errorCode' added and handles missing structures gracefully.\n","    \"\"\"\n","    # Define expected schema inside the JSON string\n","    exception_schema = StructType([\n","        StructField(\"errorCode\", StringType(), True)\n","    ])\n","    \n","    # Check for existence and parse if present\n","    if \"serviceExceptionJson\" in input_df.columns:\n","        input_df = input_df.withColumn(\"exception_struct\", from_json(col(\"serviceExceptionJson\"), exception_schema))\n","        input_df = input_df.withColumn(\"errorCode\", col(\"exception_struct.errorCode\"))\n","        input_df = input_df.drop(\"exception_struct\", \"serviceExceptionJson\")\n","    else:\n","        input_df = input_df.withColumn(\"errorCode\", lit(None).cast(StringType()))\n","    \n","    # Return result under standard name\n","    flat_df = input_df\n","    return flat_df\n","\n","print(\"The function 'extract_service_exception_fields' has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51c69667-db10-4c9a-9cfb-d6f29902ad8f"},{"cell_type":"code","source":["#\n","# Get the dataset refresh data from the BRONZE layer\n","#\n","raw_location = f\"{bronze_file_location}{dataset_id.upper()}.json\"\n","bronze_df = spark.read.json(raw_location)\n","\n","print(f\"Bronze data from {raw_location} has been read successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e2d4ffc0-87b9-4f22-8ed0-4ca37e26e46f"},{"cell_type":"code","source":["if display_data:\n","    display(bronze_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"0aee684d-32a3-4072-b0f3-7787d62399ac"},{"cell_type":"code","source":["#\n","# Check if DataFrame has exactly 1 row and an empty dataset refresh array\n","#\n","if bronze_df.count() == 1 and bronze_df.filter(size(col(\"value\")) == 0).count() == 1:\n","    print(\"❗️ No data to process--exiting notebook gracefully.\")\n","    mssparkutils.notebook.exit(\"Completed: no-op\")\n","else:\n","    print(\" ✅ Dataset refreshes exist--continue processing.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3b254e1-f10c-4e53-8e8e-34894681ef90"},{"cell_type":"code","source":["#\n","# Parse the JSON content from the bronze layer\n","#\n","\n","# Explode the refreshes array\n","refresh_df = bronze_df.select(explode(\"value\").alias(\"refreshes\"))\n","\n","# Detect which fields exist in the nested struct\n","available_fields = refresh_df.select(\"refreshes.*\").schema.names\n","\n","# List all the fields you want to pull (if present)\n","expected_fields = [\n","    (\"startTime\", \"refreshStartTime\"),\n","    (\"endTime\", \"refreshEndTime\"),\n","    (\"id\", \"refreshId\"),\n","    (\"status\", \"refreshStatus\"),\n","    (\"requestId\", \"requestId\"),\n","    (\"extendedStatus\", \"extendedStatus\"),  # optional\n","    (\"serviceExceptionJson\", \"serviceExceptionJson\"),  # optional\n","    (\"refreshType\", \"refreshType\"),\n","    (\"refreshAttempts\", \"refreshAttempts\")  # needed for explode\n","]\n","\n","# Dynamically select available ones\n","select_exprs = [\n","    col(f\"refreshes.{src}\").alias(alias)\n","    for src, alias in expected_fields if src in available_fields\n","]\n","\n","# Always include the exploded attempts safely\n","refresh_struct_df = refresh_df.select(*select_exprs)\n","\n","# Explode the attempts array, if present\n","flat_df = flatten_refresh_attempts(refresh_struct_df)\n","\n","# Parse serviceExceptionJson, if present\n","flat_df = extract_service_exception_fields(flat_df)\n","\n","print(f\"Bronze data from {raw_location} has been extracted and transformed.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"416f1a4a-b9a6-491f-a553-2b8235f229d7"},{"cell_type":"code","source":["if display_data:\n","    display(flat_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"7cf4f0bd-d3d9-4d37-91f8-aec973979535"},{"cell_type":"code","source":["#\n","# Create the silver layer dataframe\n","# Dataset IDs in the Lakehouse are expected to be uppercase\n","#\n","\n","# Identify 'time'-related columns (case-insensitive)\n","time_columns = [c for c in flat_df.columns if \"time\" in c.lower()]\n","\n","# Cast all time columns to TimestampType\n","silver_df = flat_df\n","for c_name in time_columns:\n","    silver_df = silver_df.withColumn(c_name, col(c_name).cast(TimestampType()))\n","\n","# Add uppercase DatasetId column\n","silver_df = silver_df.withColumn(\"DatasetId\", upper(lit(dataset_id)))\n","\n","# Add create date to be used in table partitioning\n","silver_df = silver_df.withColumn(\"CreationDate\", to_date(\"refreshStartTime\"))\n","\n","# Add time identifier to join these facts to the 'time' dimension table\n","silver_df = silver_df.withColumn(\n","    \"TimeId\",\n","    concat(\n","        lpad(hour(\"refreshStartTime\").cast(\"string\"), 2, \"0\"),\n","        lpad(minute(\"refreshStartTime\").cast(\"string\"), 2, \"0\")\n","    ).cast(\"int\")\n",")\n","\n","# Reorder columns — DatasetId first, CreationDate last\n","cols = silver_df.columns\n","ordered_cols = [\"DatasetId\"] + [c for c in cols if c not in [\"DatasetId\", \"CreationDate\"]] + [\"CreationDate\"]\n","silver_df = silver_df.select(ordered_cols)\n","\n","print(f\"Silver dataframe has been created successfully with {silver_df.count()} rows.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0b17bb5c-3388-4dfe-8285-85b0a49089db"},{"cell_type":"code","source":["if display_data:\n","    display(silver_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"ba1e9055-2bd8-4ed5-a264-1f18e0a4b50d"},{"cell_type":"code","source":["#\n","# Create the partitioned Delta table in the Fabric lakehouse if it does not already exist\n","#\n","if not gold_table_exists(gold_table_name, spark):\n","    silver_df.write \\\n","    .format(\"delta\") \\\n","    .partitionBy(\"CreationDate\", \"DatasetId\") \\\n","    .mode(\"errorifexists\") \\\n","    .saveAsTable(gold_table_name)\n","\n","    print(f\"Gold table {gold_table_name} has been created successfully.\")\n","    mssparkutils.notebook.exit(\"Completed: new-table\")\n","\n","else:\n","    # Get the date to start append processing\n","    refresh_date = get_refresh_date(gold_table_name, dataset_id, silver_df, spark) \n","        \n","    # Filter silver_df data based on reresh date\n","    silver_df = silver_df.filter(col(\"CreationDate\") >= lit(refresh_date))\n","\n","    print(f\"Silver dataframe has been filtered successfully on/after {refresh_date} with {silver_df.count()} rows.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20b0f9f9-660c-4302-b38f-ce43d7770c11"},{"cell_type":"code","source":["#\n","# Configure the MERGE process\n","#\n","max_retries = 5\n","retry_delay = 60  # max delay in seconds\n","success = False  # Flag to indicate success\n","\n","match_criteria = f\"\"\"\n","        target.DatasetId = source.DatasetId AND\n","        target.refreshStartTime = source.refreshStartTime AND\n","        target.refreshId = source.refreshId AND\n","        target.attemptid = source.attemptid AND\n","        target.type = source.type\n","    \"\"\"\n","\n","# Replace all non-alphanumeric characters (except spaces) with nothing\n","match_criteria = match_criteria.replace('\\n', ' ').replace('\\r', ' ')\n","# Replace multiple spaces with a single space\n","match_criteria = re.sub(r'\\s+', ' ', match_criteria)\n","# Trim leading/trailing whitespace\n","match_criteria = match_criteria.strip()\n","\n","print(\"Successfully configured the merge process.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21b82d16-a8f6-49bb-9bdd-6e4b2ba93ee0"},{"cell_type":"code","source":["#\n","# Dynamically align the source SILVER layer to the Delta GOLD table schema for MERGE\n","#\n","target_schema = spark.table(gold_table_name).columns\n","\n","# Add missing columns\n","for col_name in target_schema:\n","    if col_name not in silver_df.columns:\n","        silver_df = silver_df.withColumn(col_name, lit(None).cast(StringType()))\n","\n","# Align column order \n","silver_df = silver_df.select(target_schema)\n","\n","print(f\"Successfully aligned the source to the {gold_table_name} gold table target schema.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"38137d85-963d-40c2-a9e1-e7de116a2a15"},{"cell_type":"code","source":["#\n","# Perform the merge to insert new and update existing records (UPSERT approach)\n","#\n","\n","# Load the target Delta table\n","delta_table = DeltaTable.forName(spark, gold_table_name)\n","\n","# Perform the MERGE operation with auto-update and insert\n","for attempt in range(max_retries):\n","    try:\n","        delta_table.alias(\"target\") \\\n","            .merge(silver_df.alias(\"source\"), match_criteria) \\\n","            .whenMatchedUpdateAll() \\\n","            .whenNotMatchedInsertAll() \\\n","            .execute()\n","        success = True\n","        break  # Exit loop if successful\n","    except Exception as e:\n","        if \"ConcurrentAppendException\" in str(e):\n","            wait_time = random.randint(1, retry_delay)\n","            print(f\"Retrying due to concurrent append conflict... Attempt {attempt + 1}, sleeping {wait_time} seconds\")\n","            time.sleep(wait_time)\n","        else:\n","            raise e  # Raise other errors\n","\n","if not success:\n","    print(f\"Merge operation for gold table {gold_table_name} failed after {max_retries} retries.\")\n","else:\n","    print(f\"Gold table {gold_table_name} has been merged successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78c430fc-6e88-4085-9297-b0988ffdb60e"},{"cell_type":"code","source":["#\n","# Write history of bronze files\n","#\n","raw_path = bronze_file_location.replace(\"*/\", '', )\n","history_path = raw_path.replace(\"Files/raw/\", \"Files/history/\")\n","mssparkutils.fs.cp(raw_path, history_path + datetime.datetime.now().strftime('%Y/%m/%d') + \"/\", True) # type: ignore\n","\n","print(f\"History data copied to {history_path} successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f7a1e9a8-7730-4d1f-b028-4d16df5b9117"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7"}],"default_lakehouse":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7","default_lakehouse_name":"FUAM_Ext_Lakehouse","default_lakehouse_workspace_id":"572c83e2-ec60-4579-9648-10234b4a30d1"},"environment":{}}},"nbformat":4,"nbformat_minor":5}