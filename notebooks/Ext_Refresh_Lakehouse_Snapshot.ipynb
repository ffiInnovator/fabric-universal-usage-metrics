{"cells":[{"cell_type":"markdown","source":["#### Refresh Lakehouse Snapshot\n","\n","##### Data ingestion strategy:\n","<mark style=\"background: #88D5FF;\">**REPLACE**</mark>\n","\n","##### Related pipeline:\n","\n","**Ext_Load_PBI_Report_Usage_E2E**\n","\n","##### Source:\n","\n","**Files** from FUAM_Ext_Lakehouse\n","\n","##### Target:\n","\n","**ALL Delta table** in FUAM_Ext_Lakehouse \n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"94e47ce9-7c61-4582-a932-9cc5ccd46d9a"},{"cell_type":"code","source":["## Parameters\n","display_data = True\n","lakehouse_name = \"FUAM_Ext_lakehouse\"\n","\n","print(\"Successfully configured all paramaters for this run.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef14b835-30e9-42ec-814b-5608501fc416"},{"cell_type":"code","source":["from pyspark.sql import SparkSession # type: ignore\n","\n","print(\"Successfully imported all packages for this notebook.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c14af2cd-e5d5-4b1e-a23f-60e43eda5ad4"},{"cell_type":"code","source":["#\n","# Create the Spark session\n","#\n","app_name = \"RefreshLakehouseSnapshot\"\n","\n","# Get the current Spark session\n","spark = SparkSession.builder \\\n","    .appName(app_name) \\\n","    .getOrCreate()\n","spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n","\n","print(f\"Spark session {app_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"292a05f7-e6e5-4433-81e3-0e2a051900ad"},{"cell_type":"code","source":["#\n","# List tables in the specified lakehouse (assuming the lakehouse name is the database name)\n","#\n","spark.catalog.setCurrentDatabase(lakehouse_name)\n","\n","# Get the list of tables as a DataFrame\n","tables_df = spark.sql(\"SHOW TABLES\")\n","\n","# Extract just the table names into a Python list\n","table_names = [row.tableName for row in tables_df.collect()]\n","\n","print(f\"The list of tables fom lakehouse {lakehouse_name} has been created successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26d22e8f-d5f7-471d-a167-f309a9ab4841"},{"cell_type":"code","source":["if display_data:\n","    display(tables_df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"abfcb8aa-93e4-46b6-8de7-773980135d3f"},{"cell_type":"code","source":["#\n","# When Power BI connects to a Fabric Lakehouse in Import mode via the SQL Analytics endpoint, it may query a snapshot of the Delta table \n","# that hasn’t yet caught up with the latest physical data update. This is particularly true when:\n","# \t•\tYou’re writing to the Lakehouse using notebooks or pipelines.\n","# \t•\tThe updates are made via overwrite or non-transactional file-level operations.\n","# \t•\tPower BI’s import query pulls from a delta table snapshot, and the _delta_log has not fully committed or compacted.\n","#\n","# ✅ Recommendation\n","# \t1.\tForce a newer snapshot via the OPTIMIZE command after your Lakehouse update step to commit a clean version.\n","#\n","for table in table_names:\n","    print(f\"Optimizing table {table} ...\")\n","    spark.sql(f\"OPTIMIZE {table}\")\n","\n","print(f\"\\nAll {len(table_names)} tables in {lakehouse_name} have been committed to a clean version successfully.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4d8e1d90-0c7b-4bcd-ba84-56976925f52c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7"}],"default_lakehouse":"a2655017-a58a-416b-b08c-c9fbbf6a8ac7","default_lakehouse_name":"FUAM_Ext_Lakehouse","default_lakehouse_workspace_id":"572c83e2-ec60-4579-9648-10234b4a30d1"}}},"nbformat":4,"nbformat_minor":5}